#!/usr/bin/env python3

"""
Script containing python test suite for SCREAM test
infrastructure. This suite should be run to confirm overall
correctness. You should run this test once in generation mode to
generate baseline results using your reference commit (common
ancestor) and once in comparison mode to compare against these
baselines using your development commit. Baseline and compare runs
will use dry-run modes so we are only comparing hypothetical shell
commands, not actually running them.

You can also do a full run which will actually execute the commands.

TODO: Add doctests to libs
"""

from utils import run_cmd, check_minimum_python_version, expect
check_minimum_python_version(3, 4)

import machines_specs

import unittest, pathlib, argparse, sys, difflib

# Globals
TEST_DIR = pathlib.Path(__file__).resolve().parent
CONFIG = {
    "machine"  : None,
    "compare"  : False,
    "generate" : False,
    "full"     : False
}

###############################################################################
def run_cmd_assert_result(test_obj, cmd, from_dir=None, expected_stat=0, env=None, verbose=False):
###############################################################################
    from_dir = pathlib.Path() if from_dir is None else from_dir
    stat, output, errput = run_cmd(cmd, from_dir=from_dir, env=env, verbose=verbose)
    if expected_stat == 0:
        expectation = "SHOULD HAVE WORKED, INSTEAD GOT STAT %s" % stat
    else:
        expectation = "EXPECTED STAT %s, INSTEAD GOT STAT %s" % (expected_stat, stat)
    msg = \
"""
COMMAND: %s
FROM_DIR: %s
%s
OUTPUT: %s
ERRPUT: %s
""" % (cmd, from_dir, expectation, output, errput)
    test_obj.assertEqual(stat, expected_stat, msg=msg)

    return output

###############################################################################
def run_cmd_store_output(test_obj, cmd, baseline_path):
###############################################################################
    baseline_path.parent.mkdir(parents=True, exist_ok=True)
    output = run_cmd_assert_result(test_obj, cmd, from_dir=TEST_DIR)
    baseline_path.write_text(output)

###############################################################################
def run_cmd_check_output(test_obj, cmd, baseline_path):
###############################################################################
    test_obj.assertTrue(baseline_path.is_file, msg="Missing baseline {}".format(baseline_path))
    output = run_cmd_assert_result(test_obj, cmd, from_dir=TEST_DIR)
    diff = difflib.unified_diff(
        baseline_path.read_text().splitlines(),
        output.splitlines(),
        fromfile=str(baseline_path),
        tofile=cmd)
    diff_output = "\n".join(diff)
    test_obj.assertEqual("", diff_output, msg=diff_output)

###############################################################################
class TestBaseOuter:
    class TestBase(unittest.TestCase):
###############################################################################

        def __init__(self, source_file, *internal_args):
            super(TestBaseOuter.TestBase, self).__init__(*internal_args)
            self._source_file = source_file
            self._cmd         = "echo {}".format(source_file)

        def get_baseline(self, machine):
            return TEST_DIR.joinpath(self._source_file).with_suffix("").joinpath(machine)

        def get_machines(self):
            return [CONFIG["machine"]] if CONFIG["machine"] else machines_specs.get_all_supported_machines()

        def test_doctests(self):
            run_cmd_assert_result(self, "python3 -m doctest {}".format(self._source_file), from_dir=TEST_DIR)

        def test_pylint(self):
            run_cmd_assert_result(self, "pylint --disable C --disable R {}".format(self._source_file), from_dir=TEST_DIR)

        def test_gen_baseline(self):
            if CONFIG["generate"]:
                machines = self.get_machines()
                for machine in machines:
                    run_cmd_store_output(self, self._cmd, self.get_baseline(machine))
            else:
                self.skipTest("Skipping dry run baseline generation")

        def test_cmp_baseline(self):
            if CONFIG["compare"]:
                machines = self.get_machines()
                for machine in machines:
                    run_cmd_check_output(self, self._cmd, self.get_baseline(machine))
            else:
                self.skipTest("Skipping dry run baseline comparison")

        def test_full(self):
            if CONFIG["full"]:
                pass
            else:
                self.skipTest("Skipping full run")

###############################################################################
class TestMachinesSpecs(TestBaseOuter.TestBase):
###############################################################################

    def __init__(self, *internal_args):
        super(TestMachinesSpecs, self).__init__("machines_specs.py", *internal_args)

###############################################################################
class TestTestAllScream(TestBaseOuter.TestBase):
###############################################################################

    def __init__(self, *internal_args):
        super(TestTestAllScream, self).__init__("test_all_scream.py", *internal_args)

###############################################################################
class TestGatherAllData(TestBaseOuter.TestBase):
###############################################################################

    def __init__(self, *internal_args):
        super(TestGatherAllData, self).__init__("gather_all_data.py", *internal_args)

###############################################################################
class TestScriptsTest(TestBaseOuter.TestBase):
###############################################################################

    def __init__(self, *internal_args):
        super(TestScriptsTest, self).__init__("scripts-tests", *internal_args)

###############################################################################
def parse_command_line(args, desc):
###############################################################################
    """
    Parse custom args for this test suite. Will delete our custom args from
    sys.argv so that only args meant for unittest remain.
    """
    help_str = \
"""
{0} [TEST] [TEST]
OR
{0} --help

\033[1mEXAMPLES:\033[0m
    \033[1;32m# Run basic pylint and doctests for everything \033[0m
    > {0}

    \033[1;32m# Run basic pylint and doctests for test_all_scream \033[0m
    > {0} TestTestAllScream

    \033[1;32m# Run pylint tests for test_all_scream \033[0m
    > {0} TestTestAllScream.test_pylint

    \033[1;32m# Do a dry-run generation for test_all_scream \033[0m
    > {0} -g TestTestAllScream

    \033[1;32m# Do a dry-run comparison for test_all_scream \033[0m
    > {0} -c TestTestAllScream

    \033[1;32m# Do a full test run of test_all_scream \033[0m
    > {0} -f -m $machine TestTestAllScream

    \033[1;32m# Do a full test run of everything \033[0m
    > {0} -f -m $machine

    \033[1;32m# Do a dry-run generation for everything \033[0m
    > {0} -g

    \033[1;32m# Do a dry-run comparison for comparison \033[0m
    > {0} -c

""".format(pathlib.Path(sys.argv[0]))

    parser = argparse.ArgumentParser(
        usage=help_str,
        description=desc,
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    parser.add_argument("-m", "--machine",
                        help="Provide machine name. This is required for full (not dry) runs")

    parser.add_argument("-g", "--generate", action="store_true",
                        help="Do a dry run with baseline generation")

    parser.add_argument("-c", "--compare", action="store_true",
                        help="Do a dry run with baseline comparison")

    parser.add_argument("-f", "--full", action="store_true",
                        help="Do a full (not dry) run")

    args, py_ut_args = parser.parse_known_args()
    sys.argv[1:] = py_ut_args

    return args

###############################################################################
def scripts_tests(machine=None, generate=False, compare=False, full=False):
###############################################################################
    # Store test params in environment
    if machine:
        expect(machines_specs.is_machine_supported(machine), "Machine {} is not supported".format(machine))
        CONFIG["machine"] = machine

    expect(not (generate and compare), "Cannot do generate and compare in the same run")
    CONFIG["compare"] = compare
    CONFIG["generate"] = generate

    if full:
        expect(machine, "Must provide a machine to do a full run")
        CONFIG["full"] = full

    unittest.main(verbosity=2)

###############################################################################
def _main_func(desc):
###############################################################################
    scripts_tests(**vars(parse_command_line(sys.argv, desc)))

if (__name__ == "__main__"):
    _main_func(__doc__)
