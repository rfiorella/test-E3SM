#!/usr/bin/env python3

"""
Script containing python test suite for SCREAM test
infrastructure. This suite should be run to confirm overall
correctness. You should run this test once in generation mode to
generate baseline results using your reference commit (common
ancestor) and once in comparison mode to compare against these
baselines using your development commit. Baseline and compare runs
will use dry-run modes so we are only comparing hypothetical shell
commands, not actually running them.

You can also do a full run which will actually execute the commands.

If you are on a batch machine, it is expected that you are on a compute node.

TODO: Add doctests to libs
"""

from utils import run_cmd, check_minimum_python_version, expect
from git_utils import get_current_head

check_minimum_python_version(3, 4)

from machines_specs import is_machine_supported, is_cuda_machine, get_all_supported_machines

import unittest, pathlib, argparse, sys, difflib

# Globals
TEST_DIR = pathlib.Path(__file__).resolve().parent
CONFIG = {
    "machine"  : None,
    "compare"  : False,
    "generate" : False,
    "full"     : False
}

###############################################################################
def run_cmd_assert_result(test_obj, cmd, from_dir=None, expect_works=True, env=None, verbose=False):
###############################################################################
    from_dir = pathlib.Path() if from_dir is None else from_dir
    stat, output, errput = run_cmd(cmd, from_dir=from_dir, env=env, verbose=verbose)
    problem = None
    if expect_works and stat != 0:
        problem = "SHOULD HAVE WORKED"
    elif not expect_works and stat == 0:
        problem = "SHOULD NOT HAVE WORKED"

    if problem is not None:
        msg = \
"""
COMMAND: %s
FROM_DIR: %s
%s
OUTPUT: %s
ERRPUT: %s
""" % (cmd, from_dir, problem, output, errput)
        test_obj.assertTrue(False, msg=msg)

    return output

###############################################################################
def run_cmd_store_output(test_obj, cmd, output_file):
###############################################################################
    output_file.parent.mkdir(parents=True, exist_ok=True)
    output = run_cmd_assert_result(test_obj, cmd, from_dir=TEST_DIR)
    head = get_current_head()
    output = output.replace(head, "CURRENT_HEAD_NORMALIZED")
    output_file.write_text(output)

###############################################################################
def run_cmd_check_baseline(test_obj, cmd, baseline_path):
###############################################################################
    test_obj.assertTrue(baseline_path.is_file(), msg="Missing baseline {}".format(baseline_path))
    output = run_cmd_assert_result(test_obj, cmd, from_dir=TEST_DIR)
    head = get_current_head()
    output = output.replace(head, "CURRENT_HEAD_NORMALIZED")
    diff = difflib.unified_diff(
        baseline_path.read_text().splitlines(),
        output.splitlines(),
        fromfile=str(baseline_path),
        tofile=cmd)
    diff_output = "\n".join(diff)
    test_obj.assertEqual("", diff_output, msg=diff_output)

###############################################################################
def test_cmake_cache_contents(test_obj, build_name, cache_var, expected_value):
###############################################################################
    cache_file = TEST_DIR.parent.joinpath("ctest-build", build_name, "CMakeCache.txt")
    test_obj.assertTrue(cache_file.is_file(), "Missing cache file {}".format(cache_file)) # pylint: disable=no-member

    grep_output = run_cmd_assert_result(test_obj, "grep {} CMakeCache.txt".format(cache_var), from_dir=cache_file.parent)
    value = grep_output.split("=")[-1]
    test_obj.assertEqual(expected_value.upper(), value.upper(),
                         msg="For CMake cache variable {}, expected value '{}', got '{}'".format(cache_var, expected_value, value))

###############################################################################
class TestBaseOuter: # Hide the base class from test scanner
    class TestBase(unittest.TestCase):
###############################################################################

        def __init__(self, source_file, cmds, *internal_args):
            super(TestBaseOuter.TestBase, self).__init__(*internal_args)
            self._source_file = source_file
            self._cmds        = list(cmds)
            self._machine     = CONFIG["machine"]
            self._compare     = CONFIG["compare"]
            self._generate    = CONFIG["generate"]
            self._full        = CONFIG["full"]

            if self._full:
                expect(self._machine is not None, "Full runs require a machine")

            self._results = TEST_DIR.joinpath("results")
            self._results.mkdir(parents=True, exist_ok=True) # pylint: disable=no-member

        def get_baseline(self, cmd, machine):
            return self._results.joinpath(self._source_file).with_suffix("").\
                joinpath(machine, self.get_cmd(cmd, machine, dry_run=False).translate(str.maketrans(" /='", "____")))

        def get_cmd(self, cmd, machine, dry_run=True):
            return "{}{}".format(cmd.replace("$machine", machine).replace("$results", str(self._results)),
                                 " --dry-run" if (dry_run and "--dry-run" not in cmd) else "")

        def get_machines(self):
            return [self._machine] if self._machine else get_all_supported_machines()

        def test_doctests(self):
            run_cmd_assert_result(self, "python3 -m doctest {}".format(self._source_file), from_dir=TEST_DIR)

        def test_pylint(self):
            run_cmd_assert_result(self, "pylint --disable C --disable R {}".format(self._source_file), from_dir=TEST_DIR)

        def test_gen_baseline(self):
            if self._generate:
                machines = self.get_machines()
                for machine in machines:
                    for cmd in self._cmds:
                        run_cmd_store_output(self, self.get_cmd(cmd, machine), self.get_baseline(cmd, machine))
            else:
                self.skipTest("Skipping dry run baseline generation")

        def test_cmp_baseline(self):
            if self._compare:
                machines = self.get_machines()
                for machine in machines:
                    for cmd in self._cmds:
                        run_cmd_check_baseline(self, self.get_cmd(cmd, machine), self.get_baseline(cmd, machine))
            else:
                self.skipTest("Skipping dry run baseline comparison")

        def test_full(self):
            if self._full:
                for cmd in self._cmds:
                    run_cmd_assert_result(self, self.get_cmd(cmd, self._machine, dry_run=False), from_dir=TEST_DIR)
            else:
                self.skipTest("Skipping full run")

###############################################################################
class TestMachinesSpecs(TestBaseOuter.TestBase):
###############################################################################

    def __init__(self, *internal_args):
        super(TestMachinesSpecs, self).__init__("machines_specs.py", [], *internal_args)

###############################################################################
class TestUtils(TestBaseOuter.TestBase):
###############################################################################

    def __init__(self, *internal_args):
        super(TestUtils, self).__init__("utils.py", [], *internal_args)

###############################################################################
class TestTestAllScream(TestBaseOuter.TestBase):
###############################################################################

    CMDS_TO_TEST = [
        "test-all-scream -m $machine -b HEAD -k",
        "test-all-scream -m $machine -b HEAD -k -t dbg",
        "test-all-scream --baseline-dir $results -c EKAT_DISABLE_TPL_WARNINGS=ON -i -m $machine --submit --dry-run", # always dry run
    ]

    def __init__(self, *internal_args):
        super(TestTestAllScream, self).__init__(
            "test_all_scream.py",
            TestTestAllScream.CMDS_TO_TEST,
            *internal_args)

    def test_dbg_details(self):
        if self._full:
            cmd = self.get_cmd("./test-all-scream -m $machine -b HEAD -k -t dbg", self._machine, dry_run=False)
            run_cmd_assert_result(self, cmd, from_dir=TEST_DIR)
            test_cmake_cache_contents(self, "full_debug", "CMAKE_BUILD_TYPE", "Debug")
            test_cmake_cache_contents(self, "full_debug", "SCREAM_DOUBLE_PRECISION", "TRUE")
            test_cmake_cache_contents(self, "full_debug", "SCREAM_FPE", "FALSE")
            if not is_cuda_machine(self._machine):
                test_cmake_cache_contents(self, "full_debug", "SCREAM_PACK_SIZE", "16")
        else:
            self.skipTest("Skipping full run")

    def test_sp_details(self):
        if self._full:
            cmd = self.get_cmd("./test-all-scream -m $machine -b HEAD -k -t sp", self._machine, dry_run=False)
            run_cmd_assert_result(self, cmd, from_dir=TEST_DIR)
            test_cmake_cache_contents(self, "full_sp_debug", "CMAKE_BUILD_TYPE", "Debug")
            test_cmake_cache_contents(self, "full_sp_debug", "SCREAM_DOUBLE_PRECISION", "FALSE")
            test_cmake_cache_contents(self, "full_sp_debug", "SCREAM_FPE", "FALSE")
            if not is_cuda_machine(self._machine):
                test_cmake_cache_contents(self, "full_debug", "SCREAM_PACK_SIZE", "16")
        else:
            self.skipTest("Skipping full run")

    def test_fpe_details(self):
        if self._full:
            if is_cuda_machine(self._machine):
                self.skipTest("Skipping FPE check on cuda")
            else:
                cmd = self.get_cmd("./test-all-scream -m $machine -b HEAD -k -t fpe", self._machine, dry_run=False)
                run_cmd_assert_result(self, cmd, from_dir=TEST_DIR)
                test_cmake_cache_contents(self, "debug_nopack_fpe", "CMAKE_BUILD_TYPE", "Debug")
                test_cmake_cache_contents(self, "debug_nopack_fpe", "SCREAM_DOUBLE_PRECISION", "TRUE")
                test_cmake_cache_contents(self, "debug_nopack_fpe", "SCREAM_FPE", "TRUE")
        else:
            self.skipTest("Skipping full run")

    def test_config_fail_captured(self):
        if self._full:
            cmd = self.get_cmd("./test-all-scream -e SCREAM_FORCE_CONFIG_FAIL=True -m $machine -b HEAD -k -t dbg", self._machine, dry_run=False)
            run_cmd_assert_result(self, cmd, from_dir=TEST_DIR, expect_works=False)
        else:
            self.skipTest("Skipping full run")

    def test_build_fail_captured(self):
        if self._full:
            cmd = self.get_cmd("./test-all-scream -e SCREAM_FORCE_BUILD_FAIL=True -m $machine -b HEAD -k -t dbg", self._machine, dry_run=False)
            run_cmd_assert_result(self, cmd, from_dir=TEST_DIR, expect_works=False)
        else:
            self.skipTest("Skipping full run")

    def test_run_fail_captured(self):
        if self._full:
            cmd = self.get_cmd("./test-all-scream -e SCREAM_FORCE_RUN_FAIL=True -m $machine -b HEAD -k -t dbg", self._machine, dry_run=False)
            run_cmd_assert_result(self, cmd, from_dir=TEST_DIR, expect_works=False)
        else:
            self.skipTest("Skipping full run")

    def test_run_diff_captured(self):
        if self._full:
            cmd = self.get_cmd("./test-all-scream -e SCREAM_FORCE_RUN_DIFF=True -m $machine -b HEAD -k -t dbg", self._machine, dry_run=False)
            run_cmd_assert_result(self, cmd, from_dir=TEST_DIR, expect_works=False)
        else:
            self.skipTest("Skipping full run")

###############################################################################
class TestGatherAllData(TestBaseOuter.TestBase):
###############################################################################

    CMDS_TO_TEST = [
        "./gather-all-data './scripts/test-all-scream -m $machine -b HEAD -k' -l -m $machine",
    ]

    def __init__(self, *internal_args):
        super(TestGatherAllData, self).__init__(
            "gather_all_data.py",
            TestGatherAllData.CMDS_TO_TEST,
            *internal_args)

###############################################################################
class TestScriptsTest(TestBaseOuter.TestBase):
###############################################################################

    def __init__(self, *internal_args):
        super(TestScriptsTest, self).__init__("scripts-tests", [], *internal_args)

###############################################################################
def parse_command_line(args, desc):
###############################################################################
    """
    Parse custom args for this test suite. Will delete our custom args from
    sys.argv so that only args meant for unittest remain.
    """
    help_str = \
"""
{0} [TEST] [TEST]
OR
{0} --help

\033[1mEXAMPLES:\033[0m
    \033[1;32m# Run basic pylint and doctests for everything \033[0m
    > {0}

    \033[1;32m# Run basic pylint and doctests for test_all_scream \033[0m
    > {0} TestTestAllScream

    \033[1;32m# Run pylint tests for test_all_scream \033[0m
    > {0} TestTestAllScream.test_pylint

    \033[1;32m# Do a dry-run generation for test_all_scream \033[0m
    > {0} -g TestTestAllScream

    \033[1;32m# Do a dry-run comparison for test_all_scream \033[0m
    > {0} -c TestTestAllScream

    \033[1;32m# Do a full test run of test_all_scream \033[0m
    > {0} -f -m $machine TestTestAllScream

    \033[1;32m# Do a full test run of everything \033[0m
    > {0} -f -m $machine

    \033[1;32m# Do a dry-run generation for everything \033[0m
    > {0} -g

    \033[1;32m# Do a dry-run comparison for comparison \033[0m
    > {0} -c

    \033[1;32m# Run every possible test. This should be done before a PR is issued \033[0m
    > {0} -g # You likely want to do this for a reference commit
    > {0} -c
    > {0} -f -m $machine

""".format(pathlib.Path(args[0]).name)

    parser = argparse.ArgumentParser(
        usage=help_str,
        description=desc,
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    parser.add_argument("-m", "--machine",
                        help="Provide machine name. This is required for full (not dry) runs")

    parser.add_argument("-g", "--generate", action="store_true",
                        help="Do a dry run with baseline generation")

    parser.add_argument("-c", "--compare", action="store_true",
                        help="Do a dry run with baseline comparison")

    parser.add_argument("-f", "--full", action="store_true",
                        help="Do a full (not dry) run")

    args, py_ut_args = parser.parse_known_args()
    sys.argv[1:] = py_ut_args

    return args

###############################################################################
def scripts_tests(machine=None, generate=False, compare=False, full=False):
###############################################################################
    # Store test params in environment
    if machine:
        expect(is_machine_supported(machine), "Machine {} is not supported".format(machine))
        CONFIG["machine"] = machine

    expect(not (generate and compare), "Cannot do generate and compare in the same run")
    CONFIG["compare"] = compare
    CONFIG["generate"] = generate

    if full:
        expect(machine, "Must provide a machine to do a full run")
        CONFIG["full"] = full

    unittest.main(verbosity=2)

###############################################################################
def _main_func(desc):
###############################################################################
    scripts_tests(**vars(parse_command_line(sys.argv, desc)))

if (__name__ == "__main__"):
    _main_func(__doc__)
